{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "70f027c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required library (uncomment if needed)\n",
        "# !pip install opensearch-py\n",
        "\n",
        "from opensearchpy import OpenSearch\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "# Suppress SSL warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "55fb2308",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_all_documents(client, index_name, output_file):\n",
        "    \"\"\"\n",
        "    Fetch all documents from an OpenSearch index and save them to a file.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenSearch client instance\n",
        "        index_name: Name of the index to fetch documents from\n",
        "        output_file: Path to the output file (will be saved as JSON)\n",
        "    \n",
        "    Returns:\n",
        "        int: Total number of documents fetched\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    scroll_size = 1000  # Number of documents per scroll\n",
        "    \n",
        "    # Initial search with scroll\n",
        "    response = client.search(\n",
        "        index=index_name,\n",
        "        body={\n",
        "            \"query\": {\"match_all\": {}},\n",
        "            \"size\": scroll_size\n",
        "        },\n",
        "        scroll='2m'  # Keep scroll context alive for 2 minutes\n",
        "    )\n",
        "    \n",
        "    scroll_id = response['_scroll_id']\n",
        "    hits = response['hits']['hits']\n",
        "    \n",
        "    # Add initial batch of documents\n",
        "    for hit in hits:\n",
        "        all_documents.append({\n",
        "            '_id': hit['_id'],\n",
        "            '_source': hit['_source']\n",
        "        })\n",
        "    \n",
        "    print(f\"Fetched {len(hits)} documents...\")\n",
        "    \n",
        "    # Continue scrolling until no more documents\n",
        "    while len(hits) > 0:\n",
        "        response = client.scroll(\n",
        "            scroll_id=scroll_id,\n",
        "            scroll='2m'\n",
        "        )\n",
        "        \n",
        "        scroll_id = response['_scroll_id']\n",
        "        hits = response['hits']['hits']\n",
        "        \n",
        "        for hit in hits:\n",
        "            all_documents.append({\n",
        "                '_id': hit['_id'],\n",
        "                '_source': hit['_source']\n",
        "            })\n",
        "        \n",
        "        if len(hits) > 0:\n",
        "            print(f\"Fetched {len(all_documents)} documents so far...\")\n",
        "    \n",
        "    # Clear the scroll context\n",
        "    client.clear_scroll(scroll_id=scroll_id)\n",
        "    \n",
        "    # Save to file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_documents, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nTotal documents fetched: {len(all_documents)}\")\n",
        "    print(f\"Data saved to: {output_file}\")\n",
        "    \n",
        "    return len(all_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3416cf27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to OpenSearch successfully!\n",
            "Cluster: 408531640850:opensearch-dev\n",
            "Version: 7.10.2\n"
          ]
        }
      ],
      "source": [
        "# OpenSearch configuration\n",
        "config = {\n",
        "    'hosts': ['https://os-dev.cwsystem.in/'],\n",
        "    'http_auth': None,  # Add ('username', 'password') if authentication is required\n",
        "    'use_ssl': True,\n",
        "    'verify_certs': False,\n",
        "    'ssl_assert_hostname': False,\n",
        "    'ssl_show_warn': False,\n",
        "    'timeout': 3000\n",
        "}\n",
        "\n",
        "# Create OpenSearch client\n",
        "client = OpenSearch(**config)\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    info = client.info()\n",
        "    print(\"Connected to OpenSearch successfully!\")\n",
        "    print(f\"Cluster: {info['cluster_name']}\")\n",
        "    print(f\"Version: {info['version']['number']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1c790f7e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched 1000 documents...\n",
            "Fetched 1974 documents so far...\n",
            "\n",
            "Total documents fetched: 1974\n",
            "Data saved to: version_data.json\n",
            "\n",
            "✓ Successfully fetched and saved 1974 documents!\n"
          ]
        }
      ],
      "source": [
        "# Fetch all documents from feb_2026_vehicle_versions index\n",
        "index_name = \"feb_2026_vehicle_versions\"\n",
        "output_file = \"version_data.json\"\n",
        "\n",
        "try:\n",
        "    total_docs = fetch_all_documents(client, index_name, output_file)\n",
        "    print(f\"\\n✓ Successfully fetched and saved {total_docs} documents!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d49bc768",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Images Data Structure ===\n",
            "Total images records: 1993\n",
            "First record sample:\n",
            "{\n",
            "  \"makeid\": 36,\n",
            "  \"versionid\": 9183,\n",
            "  \"fueltype\": \"Petrol\",\n",
            "  \"averagerating\": null,\n",
            "  \"version_name\": \"Modena S\",\n",
            "  \"model_name\": \"Levante\",\n",
            "  \"imagepath\": \"/n/cw/ec/26924/levante-exterior-right-front-three-quarter-4.png?isig=0\",\n",
            "  \"modelid\": 1090,\n",
            "  \"make_name\": \"Maserati\"\n",
            "}\n",
            "\n",
            "=== Version Data Structure ===\n",
            "Total version records: 1974\n",
            "First record sample:\n",
            "{\n",
            "  \"_id\": \"pbQDTJwBqBXe7gSKoBph\",\n",
            "  \"_source\": {\n",
            "    \"vehicle_id\": \"land_rover_discovery_metropolitan_edition_2023\",\n",
            "    \"make\": \"land rover\",\n",
            "    \"model\": \"discovery\",\n",
            "    \"version_id\": \"15625\",\n",
            "    \"version_name\": \"metropolitan edition\",\n",
            "    \"segment\": \"luxury\",\n",
            "    \"body_style\": \"suvfull-size suv\",\n",
            "    \"fuel_type\": \"diesel\",\n",
            "    \"transmission\": \"automatic (tc)\",\n",
            "    \"model_trim\": \"hse\",\n",
            "    \"version_status\": \"new\",\n",
            "    \"features\": {\n",
            "      \"ride_height_adjustment\": \"true\",\n",
            "      \"anti_lock_br\n"
          ]
        }
      ],
      "source": [
        "# Inspect the structure of both files\n",
        "print(\"=== Images Data Structure ===\")\n",
        "with open('images_data.json', 'r', encoding='utf-8') as f:\n",
        "    images_data = json.load(f)\n",
        "    print(f\"Total images records: {len(images_data)}\")\n",
        "    print(f\"First record sample:\")\n",
        "    print(json.dumps(images_data[0], indent=2)[:500])\n",
        "\n",
        "print(\"\\n=== Version Data Structure ===\")\n",
        "with open('version_data.json', 'r', encoding='utf-8') as f:\n",
        "    version_data = json.load(f)\n",
        "    print(f\"Total version records: {len(version_data)}\")\n",
        "    print(f\"First record sample:\")\n",
        "    print(json.dumps(version_data[0], indent=2)[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7ff04e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_imagepath_to_version_data(version_data_file, images_data_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Add imagepath and averagerating from images_data.json to version_data.json in the _source field.\n",
        "    \n",
        "    Args:\n",
        "        version_data_file: Path to version_data.json file\n",
        "        images_data_file: Path to images_data.json file\n",
        "        output_file: Path to save the updated data (optional, defaults to version_data_file)\n",
        "    \n",
        "    Returns:\n",
        "        dict: Statistics about the operation\n",
        "    \"\"\"\n",
        "    # Load both JSON files\n",
        "    print(\"Loading data files...\")\n",
        "    with open(version_data_file, 'r', encoding='utf-8') as f:\n",
        "        version_data = json.load(f)\n",
        "    \n",
        "    with open(images_data_file, 'r', encoding='utf-8') as f:\n",
        "        images_data = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(version_data)} version records\")\n",
        "    print(f\"Loaded {len(images_data)} image records\")\n",
        "    \n",
        "    # Create a mapping dictionary: versionid -> {imagepath, averagerating}\n",
        "    # Handle potential multiple images per version by taking the first one\n",
        "    version_to_data = {}\n",
        "    for img_record in images_data:\n",
        "        version_id = str(img_record.get('versionid', ''))\n",
        "        if version_id and version_id not in version_to_data:\n",
        "            version_to_data[version_id] = {\n",
        "                'imagepath': img_record.get('imagepath', ''),\n",
        "                'averagerating': img_record.get('averagerating')\n",
        "            }\n",
        "    \n",
        "    print(f\"Created mapping for {len(version_to_data)} unique version IDs\")\n",
        "    \n",
        "    # Add imagepath and averagerating to each record's _source field\n",
        "    matched_count = 0\n",
        "    unmatched_count = 0\n",
        "    \n",
        "    for record in version_data:\n",
        "        if '_source' in record:\n",
        "            version_id = str(record['_source'].get('version_id', ''))\n",
        "            \n",
        "            if version_id in version_to_data:\n",
        "                record['_source']['imagepath'] = version_to_data[version_id]['imagepath']\n",
        "                record['_source']['averagerating'] = version_to_data[version_id]['averagerating']\n",
        "                matched_count += 1\n",
        "            else:\n",
        "                record['_source']['imagepath'] = None\n",
        "                record['_source']['averagerating'] = None\n",
        "                unmatched_count += 1\n",
        "    \n",
        "    # Save the updated data\n",
        "    if output_file is None:\n",
        "        output_file = version_data_file\n",
        "    \n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(version_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    stats = {\n",
        "        'total_records': len(version_data),\n",
        "        'matched': matched_count,\n",
        "        'unmatched': unmatched_count,\n",
        "        'output_file': output_file\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Operation Complete!\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Total records processed: {stats['total_records']}\")\n",
        "    print(f\"Records with imagepath and averagerating added: {stats['matched']}\")\n",
        "    print(f\"Records without matching data: {stats['unmatched']}\")\n",
        "    print(f\"Updated data saved to: {stats['output_file']}\")\n",
        "    \n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d60be706",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data files...\n",
            "Loaded 1974 version records\n",
            "Loaded 1993 image records\n",
            "Created mapping for 1993 unique version IDs\n",
            "\n",
            "==================================================\n",
            "Operation Complete!\n",
            "==================================================\n",
            "Total records processed: 1974\n",
            "Records with imagepath and averagerating added: 1954\n",
            "Records without matching data: 20\n",
            "Updated data saved to: version_data_with_images.json\n"
          ]
        }
      ],
      "source": [
        "# Execute the function to add imagepath to version data\n",
        "stats = add_imagepath_to_version_data(\n",
        "    version_data_file='version_data.json',\n",
        "    images_data_file='images_data.json',\n",
        "    output_file='version_data_with_images.json'  # Save to a new file to preserve original\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "175d670e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Sample Record with ImagePath ===\n",
            "{\n",
            "  \"_id\": \"pbQDTJwBqBXe7gSKoBph\",\n",
            "  \"_source\": {\n",
            "    \"vehicle_id\": \"land_rover_discovery_metropolitan_edition_2023\",\n",
            "    \"make\": \"land rover\",\n",
            "    \"model\": \"discovery\",\n",
            "    \"version_id\": \"15625\",\n",
            "    \"version_name\": \"metropolitan edition\",\n",
            "    \"segment\": \"luxury\",\n",
            "    \"body_style\": \"suvfull-size suv\",\n",
            "    \"fuel_type\": \"diesel\",\n",
            "    \"transmission\": \"automatic (tc)\",\n",
            "    \"model_trim\": \"hse\",\n",
            "    \"version_status\": \"new\",\n",
            "    \"features\": {\n",
            "      \"ride_height_adjustment\": \"true\",\n",
            "      \"anti_lock_braking_system\": \"true\",\n",
            "      \"four_wheel_drive\": \"full-time\",\n",
            "      \"hill_hold_control\": \"true\",\n",
            "      \"limited_slip_differential\": \"false\",\n",
            "      \"brake_assist\": \"true\",\n",
            "      \"electronic_stability_program\": \"true\",\n",
            "      \"electronic_brake_force_distribution\": \"true\",\n",
            "      \"hill_descent_control\": \"true\",\n",
            "      \"traction_control_system\": \"true\",\n",
            "      \"front_suspension\": \"fully independent, double wishbones with coil springs\",\n",
            "      \"rear_brake_type\": \"ventilated disc\",\n",
            "      \"puncture_repair_k\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Verify the result - show a sample record with imagepath\n",
        "print(\"\\n=== Sample Record with ImagePath ===\")\n",
        "with open('version_data_with_images.json', 'r', encoding='utf-8') as f:\n",
        "    updated_data = json.load(f)\n",
        "    \n",
        "    # Find a record that has an imagepath\n",
        "    for record in updated_data:\n",
        "        if record.get('_source', {}).get('imagepath'):\n",
        "            print(json.dumps(record, indent=2)[:1000])\n",
        "            print(\"...\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cd140d10",
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_to_opensearch_index(client, source_file, index_name, batch_size=500):\n",
        "    \"\"\"\n",
        "    Upload documents to OpenSearch index from version_data_with_images.json.\n",
        "    Extracts only the _source content (without _id and _source wrapper).\n",
        "    \n",
        "    Args:\n",
        "        client: OpenSearch client instance\n",
        "        source_file: Path to the JSON file containing documents\n",
        "        index_name: Name of the target index in OpenSearch\n",
        "        batch_size: Number of documents per bulk upload batch\n",
        "    \n",
        "    Returns:\n",
        "        dict: Statistics about the upload operation\n",
        "    \"\"\"\n",
        "    from opensearchpy import helpers\n",
        "    \n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Starting upload to index: {index_name}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Load the source data\n",
        "    print(\"Loading source data...\")\n",
        "    with open(source_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(data)} documents from {source_file}\\n\")\n",
        "    \n",
        "    # Check if index exists\n",
        "    if client.indices.exists(index=index_name):\n",
        "        print(f\"Index '{index_name}' already exists.\")\n",
        "        response = input(\"Do you want to delete and recreate it? (yes/no): \")\n",
        "        if response.lower() in ['yes', 'y']:\n",
        "            client.indices.delete(index=index_name)\n",
        "            print(f\"Deleted existing index '{index_name}'\")\n",
        "        else:\n",
        "            print(\"Aborting upload. Index already exists.\")\n",
        "            return None\n",
        "    \n",
        "    # Create the new index\n",
        "    print(f\"Creating index '{index_name}'...\")\n",
        "    client.indices.create(index=index_name)\n",
        "    print(f\"Index '{index_name}' created successfully\\n\")\n",
        "    \n",
        "    # Prepare documents for bulk upload\n",
        "    print(\"Preparing documents for upload...\")\n",
        "    actions = []\n",
        "    \n",
        "    for record in data:\n",
        "        # Extract only the _source content\n",
        "        doc_content = record.get('_source', {})\n",
        "        \n",
        "        # Use vehicle_id as the document ID for uniqueness\n",
        "        doc_id = doc_content.get('vehicle_id', '')\n",
        "        \n",
        "        if doc_id:\n",
        "            action = {\n",
        "                '_index': index_name,\n",
        "                '_id': doc_id,\n",
        "                '_source': doc_content\n",
        "            }\n",
        "            actions.append(action)\n",
        "    \n",
        "    print(f\"Prepared {len(actions)} documents for upload\\n\")\n",
        "    \n",
        "    # Bulk upload with progress tracking\n",
        "    print(\"Starting bulk upload...\")\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    \n",
        "    # Process in batches\n",
        "    for i in range(0, len(actions), batch_size):\n",
        "        batch = actions[i:i+batch_size]\n",
        "        try:\n",
        "            success, failed = helpers.bulk(client, batch, stats_only=True, raise_on_error=False)\n",
        "            success_count += success\n",
        "            error_count += len(batch) - success\n",
        "            \n",
        "            print(f\"Progress: {i+len(batch)}/{len(actions)} documents processed \"\n",
        "                  f\"(Success: {success_count}, Errors: {error_count})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i}-{i+len(batch)}: {e}\")\n",
        "            error_count += len(batch)\n",
        "    \n",
        "    # Refresh the index to make documents searchable\n",
        "    client.indices.refresh(index=index_name)\n",
        "    \n",
        "    # Get final count from index\n",
        "    count_response = client.count(index=index_name)\n",
        "    final_count = count_response['count']\n",
        "    \n",
        "    stats = {\n",
        "        'total_documents': len(data),\n",
        "        'documents_prepared': len(actions),\n",
        "        'success_count': success_count,\n",
        "        'error_count': error_count,\n",
        "        'final_index_count': final_count,\n",
        "        'index_name': index_name\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Upload Complete!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total documents in source file: {stats['total_documents']}\")\n",
        "    print(f\"Documents prepared for upload: {stats['documents_prepared']}\")\n",
        "    print(f\"Successfully uploaded: {stats['success_count']}\")\n",
        "    print(f\"Errors: {stats['error_count']}\")\n",
        "    print(f\"Final document count in index: {stats['final_index_count']}\")\n",
        "    \n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d3e703",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Starting upload to index: mcp_version_data\n",
            "============================================================\n",
            "\n",
            "Loading source data...\n",
            "Loaded 1974 documents from final_data.json\n",
            "\n",
            "Creating index 'mcp_version_data'...\n",
            "Index 'mcp_version_data' created successfully\n",
            "\n",
            "Preparing documents for upload...\n",
            "Prepared 1974 documents for upload\n",
            "\n",
            "Starting bulk upload...\n",
            "Progress: 500/1974 documents processed (Success: 500, Errors: 0)\n",
            "Progress: 1000/1974 documents processed (Success: 1000, Errors: 0)\n",
            "Progress: 1500/1974 documents processed (Success: 1500, Errors: 0)\n",
            "Progress: 1974/1974 documents processed (Success: 1974, Errors: 0)\n",
            "\n",
            "============================================================\n",
            "Upload Complete!\n",
            "============================================================\n",
            "Total documents in source file: 1974\n",
            "Documents prepared for upload: 1974\n",
            "Successfully uploaded: 1974\n",
            "Errors: 0\n",
            "Final document count in index: 1974\n"
          ]
        }
      ],
      "source": [
        "# Upload documents to the new index\n",
        "upload_stats = upload_to_opensearch_index(\n",
        "    client=client,\n",
        "    source_file='version_data_with_images.json',\n",
        "    index_name='mcp_version_data',\n",
        "    batch_size=500\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "dbf4b2e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Verification: Checking uploaded documents\n",
            "============================================================\n",
            "\n",
            "Total documents in 'version_data_mcp': 1974\n",
            "\n",
            "Sample document structure:\n",
            "Document ID: audi_q3_40_tfsi_premium_plus_2024\n",
            "\n",
            "Document Content (_source):\n",
            "{\n",
            "  \"vehicle_id\": \"audi_q3_40_tfsi_premium_plus_2024\",\n",
            "  \"make\": \"audi\",\n",
            "  \"model\": \"q3\",\n",
            "  \"version_id\": \"18875\",\n",
            "  \"version_name\": \"40 tfsi premium plus\",\n",
            "  \"segment\": \"luxury\",\n",
            "  \"body_style\": \"suvcompact suv\",\n",
            "  \"fuel_type\": \"petrol\",\n",
            "  \"transmission\": \"automatic (dct)\",\n",
            "  \"model_trim\": \"premium plus\",\n",
            "  \"version_status\": \"new\",\n",
            "  \"features\": {\n",
            "    \"backrest_bolsters_in_out\": \"false\",\n",
            "    \"seat_base_bolsters_in_out\": \"false\",\n",
            "    \"backrest_tilt_forward_back\": \"manual\",\n",
            "    \"headrest_forward_back\": \"false\",\n",
            "    \"lumbar_forward_back\": \"false\",\n",
            "    \"memory_presets\": \"not available\",\n",
            "    \"shoulder_support_bolsters_in_out\": \"false\",\n",
            "    \"extended_thigh_support_forward_back\": \"false\",\n",
            "    \"lumbar_up_down\": \"false\",\n",
            "    \"seat_base_sliding\": \"false\",\n",
            "    \"headrest_up_down\": \"manual\",\n",
            "    \"seat_adjustment\": \"4 way\",\n",
            "    \"seat_forward_back\": \"false\",\n",
            "    \"seat_base_angle_up_down\": \"false\",\n",
            "    \"seat_height_up_down\": \"false\",\n",
            "    \"shoulder_support_forward_back\": \"false\",\n",
            "    \"doors\": \"5\",\n",
            "    \"bootspace\": \"530\",\n",
            "    \"fuel_tank_capacity\": \"62.4\",\n",
            "    \"no_of_rows\": \"2\",\n",
            "    \"seating_capacity\": \"5\",\n",
            "    \"projector_headlights\": \"false\",\n",
            "    \"puddle_lamps\": \"false\",\n",
            "    \"glovebox_lamp\": \"true\",\n",
            "    \"light_on_vanity_mirrors\": \"false\",\n",
            "    \"ambient_interior_lighting\": \"true\",\n",
            "    \"cabin_lamps_position\": \"front and rear\",\n",
            "    \"front_fog_lights\": \"led\",\n",
            "    \"clear_lens_head_lamp\": \"true\",\n",
            "    \"cornering_headlights\": \"false\",\n",
            "    \"headlight_height_adjuster\": \"true\",\n",
            "    \"xenon_headlights\": \"f\n",
            "\n",
            "... (truncated)\n",
            "\n",
            "✓ Document contains 'vehicle_id': audi_q3_40_tfsi_premium_plus_2024\n",
            "✓ Document contains 'make': audi\n",
            "✓ Document contains 'model': q3\n",
            "✓ Document contains 'imagepath': /n/cw/ec/28379/q3-exterior-right-front-three-quarter-93482.png?isig=0\n",
            "\n",
            "✓ Structure verified: Documents contain only _source content without wrapper!\n"
          ]
        }
      ],
      "source": [
        "# Verify the upload - check document count and structure\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Verification: Checking uploaded documents\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Get index stats\n",
        "index_name = 'version_data_mcp'\n",
        "count_response = client.count(index=index_name)\n",
        "print(f\"Total documents in '{index_name}': {count_response['count']}\\n\")\n",
        "\n",
        "# Fetch a sample document to verify structure\n",
        "sample_query = {\n",
        "    \"query\": {\"match_all\": {}},\n",
        "    \"size\": 1\n",
        "}\n",
        "\n",
        "response = client.search(index=index_name, body=sample_query)\n",
        "\n",
        "if response['hits']['hits']:\n",
        "    sample_doc = response['hits']['hits'][0]\n",
        "    print(\"Sample document structure:\")\n",
        "    print(f\"Document ID: {sample_doc['_id']}\")\n",
        "    print(f\"\\nDocument Content (_source):\")\n",
        "    print(json.dumps(sample_doc['_source'], indent=2)[:1500])\n",
        "    print(\"\\n... (truncated)\")\n",
        "    \n",
        "    # Verify that _source contains the vehicle data directly (not wrapped)\n",
        "    source = sample_doc['_source']\n",
        "    print(f\"\\n✓ Document contains 'vehicle_id': {source.get('vehicle_id', 'NOT FOUND')}\")\n",
        "    print(f\"✓ Document contains 'make': {source.get('make', 'NOT FOUND')}\")\n",
        "    print(f\"✓ Document contains 'model': {source.get('model', 'NOT FOUND')}\")\n",
        "    print(f\"✓ Document contains 'imagepath': {source.get('imagepath', 'NOT FOUND')}\")\n",
        "    print(f\"\\n✓ Structure verified: Documents contain only _source content without wrapper!\")\n",
        "else:\n",
        "    print(\"No documents found in index!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f02921c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
